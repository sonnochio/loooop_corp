{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99619b5b-31ab-49bc-9087-53254de17fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonny/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "import cv2\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "api_key='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'\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0e780a-1f04-48ef-b3a8-8815403661ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_usb_camera(max_index=10):\n",
    "    # Try to open a camera at each index from 0 to max_index\n",
    "    for index in range(max_index):\n",
    "        cap = cv2.VideoCapture(index)\n",
    "        if not cap.isOpened():\n",
    "            cap.release()\n",
    "            continue  # If the camera at the current index did not open, move to the next index\n",
    "\n",
    "        # If the camera opens, attempt to read a frame to verify it works\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            print(f\"Camera found at index {index}\")\n",
    "            # Display the frame to verify it's the correct camera\n",
    "            cv2.imshow(f\"Camera Index {index}\", frame)\n",
    "            cv2.waitKey(5000)  # Wait for 1 second so you can see the frame\n",
    "            cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fcad6a-d43a-43ff-8f20-a9f2f20ab1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_usb_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79ab4978-7cf1-4233-8dda-497513ae6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_ports():\n",
    "    \"\"\"\n",
    "    Test the ports and returns a tuple with the available ports and the ones that are working.\n",
    "    \"\"\"\n",
    "    non_working_ports = []\n",
    "    dev_port = 0\n",
    "    working_ports = []\n",
    "    available_ports = []\n",
    "    while len(non_working_ports) < 6: # if there are more than 5 non working ports stop the testing. \n",
    "        camera = cv2.VideoCapture(dev_port)\n",
    "        if not camera.isOpened():\n",
    "            non_working_ports.append(dev_port)\n",
    "            print(\"Port %s is not working.\" %dev_port)\n",
    "        else:\n",
    "            is_reading, img = camera.read()\n",
    "            w = camera.get(3)\n",
    "            h = camera.get(4)\n",
    "            if is_reading:\n",
    "                print(\"Port %s is working and reads images (%s x %s)\" %(dev_port,h,w))\n",
    "                working_ports.append(dev_port)\n",
    "            else:\n",
    "                print(\"Port %s for camera ( %s x %s) is present but does not reads.\" %(dev_port,h,w))\n",
    "                available_ports.append(dev_port)\n",
    "        dev_port +=1\n",
    "    return available_ports,working_ports,non_working_ports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16af113b-0068-471d-b15f-4d3f1f57208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Port 0 is working and reads images (1080.0 x 1920.0)\n",
      "Port 1 is not working.\n",
      "Port 2 is not working.\n",
      "Port 3 is not working.\n",
      "Port 4 is not working.\n",
      "Port 5 is not working.\n",
      "Port 6 is not working.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: out device of bound (0-0): 1\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 2\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 3\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 4\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 5\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 6\n",
      "OpenCV: camera failed to properly initialize!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [0], [1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8bb93e-d522-47ea-ae47-57a928b76311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This section activates the program when a face is detected\n",
    "#activation of program using openCV\n",
    "\n",
    "def face_present():\n",
    "    # Load the pre-trained Haar Cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Start the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    yes_counter = 0\n",
    "    attempt_counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # Read frames from the webcam\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "        # Increase attempt counter\n",
    "        attempt_counter += 1\n",
    "    \n",
    "        # Check if faces were detected\n",
    "        if len(faces) > 0:\n",
    "            yes_counter += 1  # Increase \"Yes\" counter if faces are detected\n",
    "    \n",
    "        # Draw rectangles around the faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "        # # Display the frame\n",
    "        # cv2.imshow('Face Detection', frame)\n",
    "    \n",
    "        # Break the loop when the user presses 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "        # Control the detection frequency to 10 times per second\n",
    "        time.sleep(max(0, 0.1 - (time.time() - start_time)))\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Check if 10 attempts have been made\n",
    "        if attempt_counter == 20:\n",
    "            # Print \"There is a face\" if 8 or more detections were \"Yes\"\n",
    "            if yes_counter >= 16:\n",
    "                # Release the VideoCapture object and close the windows\n",
    "                # cap.release(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "                return True \n",
    "            # Reset counters\n",
    "            else:\n",
    "                yes_counter = 0\n",
    "                attempt_counter = 0\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92450474-a952-4bd2-9568-ee849efcc3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5133651-3276-4447-8590-4b8459cc76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this records the voice for 5 seconds \n",
    "#num is the index of the file\n",
    "#this returns the file name\n",
    "def start_recording(num, recording_seconds=5):\n",
    "    p = pyaudio.PyAudio()\n",
    "    FORMAT = pyaudio.paInt16  # Format of audio samples (16-bit signed integers)\n",
    "    CHANNELS = 1              # Number of audio channels (1 for mono, 2 for stereo)\n",
    "    RATE = 44100              # Sample rate (samples per second)\n",
    "    CHUNK = 1024              # Number of frames per buffer\n",
    "    RECORD_SECONDS = recording_seconds        # Duration of recording in seconds\n",
    "    \n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    \n",
    "    print(\"Recording finished.\")\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    WAVE_OUTPUT_FILENAME = f\"./audio/test_{num}.wav\"\n",
    "    \n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    print(\"Recording saved as\", WAVE_OUTPUT_FILENAME)\n",
    "    \n",
    "    return WAVE_OUTPUT_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2bc9d5-b986-44dd-b3d4-8ef70416cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_recording(num, recording_seconds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333c7680-a5e2-4764-879d-9076819ecc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this transforms speech to text\n",
    "#this already does the job\n",
    "def stt (num, recording_seconds=5):\n",
    "    api_key='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'\n",
    "    start_recording(num,recording_seconds)\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    file_name=f\"./audio/test_{num}.wav\"\n",
    "    audio_file= open(file_name, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file)\n",
    "\n",
    "    return (file_name,transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377db1b-5dba-4c42-adc3-017ba12fd611",
   "metadata": {},
   "source": [
    "# Video jumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7252d05-c2d2-4124-a8c8-45785eae035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='./loooop_test.mp4'\n",
    "start_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c65bc2-a0bc-408b-b0a4-ca2af4140054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video_segment(video_path, start_time, duration, with_audio=True):\n",
    "    \"\"\"\n",
    "    Plays a segment of the video.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file.\n",
    "    - start_time: Start time of the segment in seconds.\n",
    "    - duration: Duration of the segment to play in seconds.\n",
    "    - with_audio: Whether to play with audio or not.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_duration = frame_count / fps\n",
    "\n",
    "    # Set the starting frame\n",
    "    start_frame = int(start_time * fps)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    # Calculate the end time\n",
    "    end_time = start_time + duration\n",
    "    if end_time > total_duration:\n",
    "        end_time = total_duration\n",
    "\n",
    "    current_time = start_time\n",
    "    while cap.isOpened() and current_time < end_time:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        current_time += 1/fps\n",
    "\n",
    "        # Here you would control the audio playback based on the 'with_audio' flag.\n",
    "        # This might involve using an external library for audio control, as OpenCV does not handle audio.\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5850e21e-a3f1-46a8-932a-7c371819a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_video_segment(video_path,start_time,duration=5,with_audio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a932f-602c-4714-8691-d955f3a727de",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c446e262-61be-481a-b5a7-6f71d52c4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system question prompt to users \n",
    "\n",
    "prompts=\"\"\"What’s your name?|What do you do for a living?|Can you describe a moment in your life where you felt completely at peace or truly happy? (5 - second)|Can you describe a challenge you've recently faced and how you approached it?|What's one piece of advice you've received that has profoundly impacted your life? How have you applied it?|How much did you spend on the last piece of clothing?|( Checkout section ) what’s your address?\"\"\"\n",
    "prompt_ls=prompts.split('|')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f00b8e-cac8-4304-8c43-404a856f533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What’s your name?',\n",
       " 'What do you do for a living?',\n",
       " 'Can you describe a moment in your life where you felt completely at peace or truly happy? (5 - second)',\n",
       " \"Can you describe a challenge you've recently faced and how you approached it?\",\n",
       " \"What's one piece of advice you've received that has profoundly impacted your life? How have you applied it?\",\n",
       " 'How much did you spend on the last piece of clothing?',\n",
       " '( Checkout section ) what’s your address?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c80d8b-d7e1-4beb-b531-b5e82ba95f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #different recording secs for diff questions\n",
    "# #prompt with some example answers\n",
    "# if face_present() is True:\n",
    "#     start_recording(num, recording_seconds=10)\n",
    "#     db=[]\n",
    "#     for num,prompt in enumerate(prompt_ls):\n",
    "#         print(\"\\n\",\"--------------------------------------------------------\",\"\\n\", f\"❓ {prompt}\", \"\\n\",\"\\n\")\n",
    "#         transcript=stt(num,recording_seconds=10)[1]\n",
    "#         print(\"\\n\", \"🫵\",transcript.text,\"\\n\", \"--------------------------------------------------------\",\"\\n\")\n",
    "#         db.append(f\"Q:{prompt},A:{transcript.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6cddf55-0c19-408b-a370-563da0390906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# api_key='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'\n",
    "# client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "\n",
    "def create_rumors(db):\n",
    "    rumour_system_promt=f\"\"\"I am working on an art project. Based on the questions and answers from users below , can you generate some shocking news that are untrue about this person. or alter the answers to generate rumors that are opposite to what they say. You should generate 5 facts or news or rumours that demonstrate using this person's data against them and generate untrue information. The 5 facts should be 5 sentences that are 10 - 20 words each. \n",
    "\n",
    "You should output the 5 sentences in the following format :\n",
    "\n",
    "\"art project starts\"|sentence 1| sentence 2|sentence 3|sentence 4|sentence 5\n",
    "\n",
    "Below are the list of questions and answers:  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"{rumour_system_promt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{db}\"},\n",
    "      ]\n",
    "    )\n",
    "    rumors=response.choices[0].message.content.split('|')[1:]\n",
    "    print(rumors)\n",
    "    return rumors\n",
    "# create_rumors(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca69ce-eac1-4131-85d7-1a89bc469c2e",
   "metadata": {},
   "source": [
    "# Webcam capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1ce234-870f-4d38-b017-8b1eb8d9caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##capture an image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def take_picture_with_webcam(id):\n",
    "    # Open a connection to the webcam (0 is usually the default webcam)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    " \n",
    "    # Check if the webcam is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open webcam.\")\n",
    "        return None\n",
    "\n",
    "    cv2.waitKey(500)\n",
    "\n",
    "    # Capture a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the webcam connection\n",
    "    cap.release()\n",
    "\n",
    "    # Check if the frame is captured successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        return None\n",
    "\n",
    "    # Define the directory to save pictures\n",
    "    pictures_dir = \"pictures\"\n",
    "    if not os.path.exists(pictures_dir):\n",
    "        os.makedirs(pictures_dir)\n",
    "\n",
    "    # Generate a unique identifier for the picture filename\n",
    "    while os.path.exists(os.path.join(pictures_dir, f\"{id}.png\")):\n",
    "        id += 1\n",
    "\n",
    "    # Save the captured frame as an image\n",
    "    picture_path = os.path.join(pictures_dir, f\"{id}.png\")\n",
    "    \n",
    "\n",
    "    \n",
    "    cv2.imwrite(picture_path, frame)\n",
    "\n",
    "    #crop to be square\n",
    "    with Image.open(picture_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "        # Determine the size of the square (use the smaller dimension)\n",
    "        new_size = min(width, height)\n",
    "\n",
    "        # Calculate the left, upper, right, and lower pixels to crop\n",
    "        left = (width - new_size)/2\n",
    "        top = (height - new_size)/2\n",
    "        right = (width + new_size)/2\n",
    "        bottom = (height + new_size)/2\n",
    "\n",
    "        # Perform the crop\n",
    "        img_cropped = img.crop((left, top, right, bottom))\n",
    "        crop_path = f\"./crop/{id}.png\"\n",
    "        img_cropped.save(crop_path)\n",
    "        img_cropped.show()\n",
    "\n",
    "    print(f\"Cropped picture saved successfully: {crop_path}\")\n",
    "\n",
    "    return img_cropped, crop_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b6d2caa-8a8b-4272-8359-114250c85fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captured_image, image_path = take_picture_with_webcam()\n",
    "# print(\"Image path:\", image_path)\n",
    "\n",
    "# #Import image\n",
    "# image = cv2.imread(image_path)\n",
    "# image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# #Show the image with matplotlib\n",
    "# plt.imshow(image_rgb)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9296b4f-d14f-4778-9742-45f9b160b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def create_face_mask(image_path, id):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Load OpenCV's pre-trained Haar Cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # Create an all-black mask\n",
    "    mask = np.zeros((h, w), dtype=\"uint8\")\n",
    "\n",
    "    # Loop over the faces and draw white rectangles on the mask\n",
    "    for (x, y, w, h) in faces:\n",
    "        mask[y:y+h, x:x+w] = 255\n",
    "\n",
    "    # Create a 4-channel image (BGRA) by adding the mask as the alpha channel\n",
    "    image_with_alpha = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
    "    image_with_alpha[:, :, 3] = mask\n",
    "\n",
    "    # Save the result\n",
    "    output_path = f'./masks/{id}.png'\n",
    "    cv2.imwrite(output_path, image_with_alpha)\n",
    "\n",
    "    crop_mask_path=f'./crop_mask/{id}.png'\n",
    "    \n",
    "    with Image.open(output_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "        # Determine the size of the square (use the smaller dimension)\n",
    "        new_size = min(width, height)\n",
    "\n",
    "        # Calculate the left, upper, right, and lower pixels to crop\n",
    "        left = (width - new_size)/2\n",
    "        top = (height - new_size)/2\n",
    "        right = (width + new_size)/2\n",
    "        bottom = (height + new_size)/2\n",
    "\n",
    "        # Perform the crop\n",
    "        img_cropped = img.crop((left, top, right, bottom))\n",
    "        crop_path = f\"./crop_mask/{id}.png\"\n",
    "        img_cropped.save(crop_mask_path)\n",
    "        img_cropped.show()\n",
    "        \n",
    "    return crop_mask_path\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5dd2b6-477c-4c05-a61c-3fd7dc9339d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbdd2665-1df2-4415-a90a-5391bf645d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #different recording secs for diff questions\n",
    "# #prompt with some example answers\n",
    "# if face_present() is True:\n",
    "#     db=[]\n",
    "#     id=1\n",
    "#     for num,prompt in enumerate(prompt_ls):\n",
    "#         print(\"\\n\",\"--------------------------------------------------------\",\"\\n\", f\"❓ {prompt}\", \"\\n\",\"\\n\")\n",
    "#         transcript=stt(num,recording_seconds=15)[1]\n",
    "#         print(\"\\n\", \"🫵\",transcript.text,\"\\n\", \"--------------------------------------------------------\",\"\\n\")\n",
    "#         db.append(f\"Q:{prompt},A:{transcript.text}\")\n",
    "    \n",
    "#     #creates rumors\n",
    "#     rumors=create_rumors(db)\n",
    "    \n",
    "#     #takes picture\n",
    "#     captured_image, image_path = take_picture_with_webcam(id)\n",
    "#     print(\"Image path:\", image_path)\n",
    "    \n",
    "#     #Import image\n",
    "#     image = cv2.imread(image_path)\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "\n",
    "\n",
    "#     #generate mask\n",
    "#     masked_image_path = create_face_mask(image_path,1)\n",
    "#     print(f\"Masked image saved to: {masked_image_path}\")\n",
    "\n",
    "#     #display pictures\n",
    "#     im1 = Image.open(image_path)\n",
    "#     im1.show()\n",
    "    \n",
    "#     im2 = Image.open(masked_image_path)\n",
    "#     im2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d0d5be-533a-4e7d-8186-1e63c720a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rumors= \"Leo is not actually a student but claims to be to avoid work.\\nLeo never feels at peace or truly happy, it's all an act.\\nLeo never faces challenges head-on and gives up easily.\\nLeo never applies deep advice to his life; he just pretends to understand.\\nLeo actually spent 500 pounds on his last clothing purchase, not 180 as stated.\"\n",
    "# rumors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21dd8496-4126-4879-94e0-a34c94e9fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rumorimage prompt creation \n",
    "def create_rumor_image_prompt(rumors):\n",
    "    rumor_image_prompt_response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. Your job is to create a prompt for the image generating AI model Dalle-2 based on the user content. Your output should be useful to generate a picture. You can pick one setence or the all the sentences to focus on from the user content, in order to create a compelling image story. You should specify a place, a time, a color scheme, the emotions, the texture. the style of the image should be realistic \"},\n",
    "        {\"role\": \"user\", \"content\": f\"{rumors}\"},\n",
    "  ]\n",
    "    )\n",
    "    rumor_image_prompt=rumor_image_prompt_response.choices[0].message.content\n",
    "\n",
    "    return rumor_image_prompt\n",
    "\n",
    "def summarize_image_prompt(rumor_image_prompt):\n",
    "    summary_prompt_response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are an AI assist, you job is to summarize the user prompt into 1 sentence that describes the story within. It should always contain the person's name, the action and the context. This should be between 10-20 words.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{rumor_image_prompt}\"},\n",
    "  ]\n",
    "    )\n",
    "    print(summary_prompt_response)\n",
    "    summary_prompt_response=summary_prompt_response.choices[0].message.content\n",
    "    print(summary_prompt_response)\n",
    "    return summary_prompt_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "423c6f5d-c95d-413a-9bbe-8feacd8dec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8xvQyDs0cwaxAsPxzWnUezKvQlWVN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Emily displays secretive identity in a dimly lit restaurant, wearing luxurious attire, displeased while hoarding food, captured in moody colors.', role='assistant', function_call=None, tool_calls=None))], created=1709292820, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=28, prompt_tokens=173, total_tokens=201))\n",
      "Emily displays secretive identity in a dimly lit restaurant, wearing luxurious attire, displeased while hoarding food, captured in moody colors.\n"
     ]
    }
   ],
   "source": [
    "test_prompt=\" A realistic image set in a dimly lit, moody restaurant during dinner time. Capture a woman who, despite being known as Emily, is portrayed as someone with a secretive identity, possibly wearing luxurious, extravagant attire. Focus on her expression of displeasure while eating, reflecting inner turmoil rather than enjoyment. In the background, subtly hint at food hoarding with overflowing plates of untouched dishes. Incorporate a color scheme of deep blues and purples to convey a sense of mystery and unhappiness. The image style should depict realism with a touch of melancholy.\"\n",
    "summarized=summarize_image_prompt(test_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70c9b20d-811e-4395-932e-7efa7ebe0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily displays secretive identity in a dimly lit restaurant, wearing luxurious attire, displeased while hoarding food, captured in moody colors.\n"
     ]
    }
   ],
   "source": [
    "print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077a6d0e-e973-4726-ad6a-90de6cde402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rumor image creation\n",
    "\n",
    "\n",
    "def create_image(rumor_image_prompt):\n",
    "    \n",
    "    response = client.images.edit(\n",
    "      model=\"dall-e-2\",\n",
    "      image=open(image_path, \"rb\"),\n",
    "      mask=open(masked_image_path, \"rb\"),\n",
    "      prompt=rumor_image_prompt,\n",
    "      n=1,\n",
    "      size=\"512x512\"\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    return image_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b723bfdd-4fdd-42c5-81c0-e849195cb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ What’s your name? \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_0.wav\n",
      "\n",
      " 🫵 E-e-e-emily, Emily. \n",
      " -------------------------------------------------------- \n",
      "\n",
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ What do you do for a living? \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_1.wav\n",
      "\n",
      " 🫵 사는, 사는 것들, 사는 것들을 사는 것들, 비싼 것들을 사는 것들 \n",
      " -------------------------------------------------------- \n",
      "\n",
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ Can you describe a moment in your life where you felt completely at peace or truly happy? (5 - second) \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_2.wav\n",
      "\n",
      " 🫵 I feel completely happy when I eat a lot of food. \n",
      " -------------------------------------------------------- \n",
      "\n",
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ Can you describe a challenge you've recently faced and how you approached it? \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_3.wav\n",
      "\n",
      " 🫵 A recent challenge that I faced, including having no food to eat. \n",
      " -------------------------------------------------------- \n",
      "\n",
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ What's one piece of advice you've received that has profoundly impacted your life? How have you applied it? \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_4.wav\n",
      "\n",
      " 🫵 Always have stuff in your fridge, do groceries beforehand. \n",
      " -------------------------------------------------------- \n",
      "\n",
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ How much did you spend on the last piece of clothing? \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_5.wav\n",
      "\n",
      " 🫵 Around 6 pounds, 6 pounds for a t-shirt. \n",
      " -------------------------------------------------------- \n",
      "\n",
      "\n",
      " -------------------------------------------------------- \n",
      " ❓ ( Checkout section ) what’s your address? \n",
      " \n",
      "\n",
      "Recording...\n",
      "Recording finished.\n",
      "Recording saved as ./audio/test_6.wav\n",
      "\n",
      " 🫵 Uh, uh, Guinness, Guinness Street \n",
      " -------------------------------------------------------- \n",
      "\n",
      "[\"Emily spends all her money on expensive things she doesn't need.\\nEmily finds peace in starvation and not being able to eat. \\nEmily lives in poverty on Guinness Street.\\nEmily's happiness depends on material possessions.\\nEmily's fridge is always empty of food.\"]\n",
      "Cropped picture saved successfully: ./crop/13.png\n",
      "Image path: ./crop/13.png\n",
      "Masked image saved to: ./crop_mask/1.png\n",
      "ChatCompletion(id='chatcmpl-8xvT4euLpLA15vdW7lgOoDJeI7jE2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"The Paradox of Emily\\'s World\": Emily\\'s conflicting emotions are shown through her opulent possessions and stark living conditions in a dim apartment.', role='assistant', function_call=None, tool_calls=None))], created=1709292950, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=28, prompt_tokens=246, total_tokens=274))\n",
      "\"The Paradox of Emily's World\": Emily's conflicting emotions are shown through her opulent possessions and stark living conditions in a dim apartment.\n",
      "\"The Paradox of Emily's World\": Emily's conflicting emotions are shown through her opulent possessions and stark living conditions in a dim apartment.\n"
     ]
    }
   ],
   "source": [
    "#different recording secs for diff questions\n",
    "#prompt with some example answers\n",
    "if face_present() is True:\n",
    "    db=[]\n",
    "    id=1\n",
    "    for num,prompt in enumerate(prompt_ls):\n",
    "        print(\"\\n\",\"--------------------------------------------------------\",\"\\n\", f\"❓ {prompt}\", \"\\n\",\"\\n\")\n",
    "        transcript=stt(num,recording_seconds=10)[1]\n",
    "        print(\"\\n\", \"🫵\",transcript.text,\"\\n\", \"--------------------------------------------------------\",\"\\n\")\n",
    "        db.append(f\"Q:{prompt},A:{transcript.text}\")\n",
    "    \n",
    "    #creates rumors\n",
    "    rumors=create_rumors(db)\n",
    "    \n",
    "    #takes picture\n",
    "    captured_image, image_path = take_picture_with_webcam(id=1)\n",
    "    print(\"Image path:\", image_path)\n",
    "    \n",
    "    #Import image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "\n",
    "\n",
    "    #generate mask\n",
    "    masked_image_path = create_face_mask(image_path,1)\n",
    "    print(f\"Masked image saved to: {masked_image_path}\")\n",
    "\n",
    "    #display pictures\n",
    "    im1 = Image.open(image_path)\n",
    "    im1.show()\n",
    "    \n",
    "    im2 = Image.open(masked_image_path)\n",
    "    im2.show()\n",
    "\n",
    "    #create rumor image prompts \n",
    "    rumor_image_prompt=create_rumor_image_prompt(rumors)\n",
    "    summarized_image_prompt=summarize_image_prompt(rumor_image_prompt)\n",
    "    print(summarized_image_prompt)\n",
    "\n",
    "    #create rumor image\n",
    "    image_url=create_image(rumor_image_prompt)\n",
    "\n",
    "    #open rumor image\n",
    "    response_rumor = requests.get(image_url)\n",
    "    img_rumor = Image.open(BytesIO(response_rumor.content))\n",
    "    \n",
    "    img_rumor.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2f46d-9337-4bbb-bad9-6440edab89d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1462ff7-3b99-41ed-8503-3defb301ad70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdabb181-c461-4118-b50d-2e2fa25f2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creates rumors\n",
    "# rumors=create_rumors(db)\n",
    "\n",
    "# #takes picture\n",
    "# captured_image, image_path = take_picture_with_webcam(id=1)\n",
    "# print(\"Image path:\", image_path)\n",
    "\n",
    "# #Import image\n",
    "# image = cv2.imread(image_path)\n",
    "# image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\n",
    "# #generate mask\n",
    "# masked_image_path = create_face_mask(image_path,1)\n",
    "# print(f\"Masked image saved to: {masked_image_path}\")\n",
    "\n",
    "# #display pictures\n",
    "# im1 = Image.open(image_path)\n",
    "# im1.show()\n",
    "\n",
    "# im2 = Image.open(masked_image_path)\n",
    "# im2.show()\n",
    "\n",
    "# #create rumor image prompts \n",
    "# rumor_image_prompt=create_rumor_image_prompt(rumors)\n",
    "\n",
    "# #create rumor image\n",
    "# image_url=create_image(rumor_image_prompt)\n",
    "\n",
    "# #open rumor image\n",
    "# response_rumor = requests.get(image_url)\n",
    "# img_rumor = Image.open(BytesIO(response_rumor.content))\n",
    "\n",
    "# img_rumor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83052109-d88c-4ef6-9170-5132aa095d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-MpBZYN8l81PrQVI1P9ApFNTS/user-NFdu7pIXlOObW7Dy8m5ZGYQa/img-t1gWVQh4tEBv1gB6zpJZQkVW.png?st=2024-02-19T19%3A57%3A29Z&se=2024-02-19T21%3A57%3A29Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-02-19T18%3A21%3A10Z&ske=2024-02-20T18%3A21%3A10Z&sks=b&skv=2021-08-06&sig=W7F4d%2BH8m6C6XFl/cTtbNw84hZHeasobMqUIgk4LKW0%3D'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3a56519-347b-4f9a-af1f-0b81d435ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_rumor = requests.get(image_url)\n",
    "img_rumor = Image.open(BytesIO(response_rumor.content))\n",
    "\n",
    "img_rumor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6265ac-9859-480d-be4b-3806b4265ce3",
   "metadata": {},
   "source": [
    "# miscellaneous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3a7afb4-41dd-42a3-b190-bbe7e2acb362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/p9l97bzs58n4xb5ss32_jn480000gn/T/ipykernel_25503/2705022837.py:10: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "for num,i in enumerate(prompt_ls):\n",
    "    speech_file_path = f\"./audio/prompt_audio/{num}.mp3\"\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=i\n",
    "    )\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7137d32-5548-4ed3-b18b-d078b658e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/p9l97bzs58n4xb5ss32_jn480000gn/T/ipykernel_25503/2582989410.py:7: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "#loop corp welcome\n",
    "\n",
    "speech_file_path = \"./audio/prompt_audio/welcome.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=\"Welcome to Loop Corp - a completely automated shopping experience! Tell us a bit more about yourself and we will find the perfect product for you! \"\n",
    "    )\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be6a3e51-7a1e-485c-9072-c2ca6e1882f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/p9l97bzs58n4xb5ss32_jn480000gn/T/ipykernel_8872/3711107654.py:8: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "#loop corp end\n",
    "speech_file_path = \"./audio/prompt_audio/goodbye.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=\"THANKS FOR SHOPPING WITH LOOP CORP!! Your order will be with you shortly. Please don't forget to complete the survey on your way out to help us provide better customer expereince. We look forward to seeing you again! \"\n",
    "    )\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c753c8-a07e-4976-885f-22b67eff6490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "speech_file_path = f\"./audio/prompt_audio/{num}.mp3\"\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=i\n",
    "    )\n",
    "    response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60946920-3e75-4e41-8054-0068b9c9519a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What’s your name?',\n",
       " 'What do you do for a living?',\n",
       " 'Can you describe a moment in your life where you felt completely at peace or truly happy? (5 - second)',\n",
       " \"Can you describe a challenge you've recently faced and how you approached it?\",\n",
       " \"What's one piece of advice you've received that has profoundly impacted your life? How have you applied it?\",\n",
       " 'How much did you spend on the last piece of clothing?',\n",
       " '( Checkout section ) what’s your address?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad4a52-936a-4504-9ee2-a92342485696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Loop Corp\" is an avant-garde interactive exhibition that delves into the shadows of AI training data collection and its potential misuse against unsuspecting individuals. By engaging participants in a faux personal shopping experience set within a meticulously designed shop-like environment, \"Loop\" collects responses to seemingly innocuous questions. These responses, however, serve as fodder for GPT-powered algorithms that generate rumors and divisive content, laying bare the mechanisms of involuntary data extraction and its consequences. This project not only explores the ethical boundaries of AI and data privacy but also simulates the real-world implications of such technologies on personal and societal levels. Through this immersive experience, \"Loop\" aims to provoke thought, debate, and awareness among its participants, highlighting the urgent need for responsible AI development and data handling practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
